# Toolcase System Architecture

> Replicable blueprint for the extensible AI tool system.
> Use this document to rebuild or replicate the tool infrastructure.

---

## Overview

The tool system provides a type-safe, extensible framework for creating tools that AI agents can invoke. Key features:

- **Type-safe parameters** via Pydantic generics
- **Standardized error handling** with error codes
- **Built-in caching** with TTL support
- **Async/sync interoperability**
- **Streaming progress** for long-running operations
- **LangChain integration** for agent execution
- **Central registry** for tool discovery

---

## Directory Structure

```
app/tools/
â”œâ”€â”€ __init__.py              # Public exports and init_tools()
â”œâ”€â”€ base.py                  # BaseTool, ToolRegistry, ToolMetadata, ToolError
â”œâ”€â”€ cache.py                 # ToolCache with TTL support
â”‚
â”œâ”€â”€ discovery/               # Meta-tool for listing available tools
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ discovery_tool.py
â”‚
â”œâ”€â”€ [category]/              # Group tools by category
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ [tool_name].py
â”‚
â””â”€â”€ ... (other categories)
```

---

## Core Components

### 1. base.py - Foundation Classes

```python
"""Base tool class and registry for the tool system."""
import asyncio
import traceback
from abc import ABC, abstractmethod
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field
from typing import AsyncIterator, Callable, ClassVar, Coroutine, Generic, Literal, TypeVar

from langchain_core.tools import StructuredTool
from pydantic import BaseModel, ConfigDict, Field

from ..logging import get_logger

log = get_logger(__name__)

from .cache import get_tool_cache, DEFAULT_TTL_SECONDS


# ============================================
# Progress Streaming Types
# ============================================

@dataclass
class ToolProgress:
    """Progress event emitted during tool execution.
    
    These events provide real-time updates on long-running tool operations,
    allowing the UI to display meaningful progress to users.
    """
    kind: Literal["status", "step", "source_found", "data", "complete", "error"]
    message: str = ""
    step_number: int | None = None
    total_steps: int | None = None
    percentage: float | None = None
    data: dict | None = field(default_factory=dict)
    
    def to_dict(self) -> dict:
        """Convert to dict for SSE serialization."""
        result = {"kind": self.kind, "message": self.message}
        if self.step_number is not None:
            result["step_number"] = self.step_number
        if self.total_steps is not None:
            result["total_steps"] = self.total_steps
        if self.percentage is not None:
            result["percentage"] = self.percentage
        if self.data:
            result["data"] = self.data
        return result


ProgressCallback = Callable[[ToolProgress], None]


# ============================================
# Tool Metadata
# ============================================

class ToolMetadata(BaseModel):
    """Metadata for tool discovery and configuration."""
    name: str                           # Unique tool identifier (snake_case)
    description: str                    # Description shown to LLM for tool selection
    category: str                       # Grouping category (e.g., "search", "career", "memory")
    requires_api_key: bool = False      # Whether tool needs external API key
    enabled: bool = True                # Can disable tools without removing
    supports_streaming: bool = False    # Whether tool supports progress streaming


# ============================================
# Standardized Error Handling
# ============================================

class ToolError(BaseModel):
    """Standardized error response for tool failures.
    
    Use this model to return consistent error information across all tools.
    The `recoverable` field indicates whether the caller should retry or
    try an alternative approach.
    """
    error: bool = True
    message: str = Field(..., description="Human-readable error message")
    tool_name: str = Field(..., description="Name of the tool that encountered the error")
    recoverable: bool = Field(
        default=True, 
        description="Whether this error is recoverable (retry or alternative approach possible)"
    )
    error_code: str = Field(
        default="UNKNOWN_ERROR",
        description="Machine-readable error code for programmatic handling"
    )
    details: str | None = Field(
        default=None,
        description="Additional error details or stack trace for debugging"
    )
    
    def to_string(self) -> str:
        """Convert error to formatted string for LLM consumption."""
        parts = [f"**Tool Error ({self.tool_name}):** {self.message}"]
        if self.recoverable:
            parts.append("_This error may be recoverable - consider retrying or trying an alternative approach._")
        if self.details:
            parts.append(f"\nDetails: {self.details}")
        return "\n".join(parts)


class EmptyParams(BaseModel):
    """Default empty params for tools with no required inputs."""
    pass


# Standard error codes for consistent error handling
class ErrorCodes:
    """Standard error codes for tool failures."""
    API_KEY_MISSING = "API_KEY_MISSING"
    API_KEY_INVALID = "API_KEY_INVALID"
    RATE_LIMITED = "RATE_LIMITED"
    NETWORK_ERROR = "NETWORK_ERROR"
    TIMEOUT = "TIMEOUT"
    INVALID_PARAMS = "INVALID_PARAMS"
    NO_RESULTS = "NO_RESULTS"
    PARSE_ERROR = "PARSE_ERROR"
    EXTERNAL_SERVICE_ERROR = "EXTERNAL_SERVICE_ERROR"
    UNKNOWN_ERROR = "UNKNOWN_ERROR"


# ============================================
# Base Tool Class
# ============================================

TParams = TypeVar("TParams", bound=BaseModel)


class BaseTool(ABC, BaseModel, Generic[TParams]):
    """Abstract base class for all tools.
    
    Tools must implement:
    - `_run(params)`: Synchronous execution with typed params
    - `metadata`: Class variable with tool info
    - `params_schema`: Class variable defining the params type
    
    For async tools, implement _async_run and use the helper:
        def _run(self, params: TParams) -> str:
            return self._run_async_in_sync(self._async_run(params))
    
    Error handling:
        Use `_create_error()` to return standardized error responses:
        return self._create_error("Something went wrong", ErrorCodes.NETWORK_ERROR)
    
    Caching:
        Results are cached by default with a 5-minute TTL to prevent repeated
        API calls for identical queries. Override class variables to customize:
        - `cache_enabled = False` to disable caching for a tool
        - `cache_ttl = 60` to set a custom TTL in seconds
    """
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    # Class variables - override in subclasses
    metadata: ClassVar[ToolMetadata]
    params_schema: ClassVar[type[BaseModel]] = EmptyParams
    
    # Caching configuration - override in subclasses as needed
    cache_enabled: ClassVar[bool] = True
    cache_ttl: ClassVar[float] = DEFAULT_TTL_SECONDS  # 5 minutes
    
    # ============================================
    # Error Handling Methods
    # ============================================
    
    def _create_error(
        self,
        message: str,
        error_code: str = ErrorCodes.UNKNOWN_ERROR,
        recoverable: bool = True,
        include_trace: bool = False,
    ) -> str:
        """Create a standardized error response.
        
        Args:
            message: Human-readable error message
            error_code: Machine-readable error code from ErrorCodes
            recoverable: Whether the caller should retry or try alternatives
            include_trace: Whether to include stack trace in details
        
        Returns:
            Formatted error string for LLM consumption
        """
        details = None
        if include_trace:
            details = traceback.format_exc()
        
        error = ToolError(
            message=message,
            tool_name=self.metadata.name,
            error_code=error_code,
            recoverable=recoverable,
            details=details,
        )
        return error.to_string()
    
    def _create_error_from_exception(
        self,
        e: Exception,
        context: str = "",
        recoverable: bool = True,
    ) -> str:
        """Create a standardized error response from an exception.
        
        Args:
            e: The caught exception
            context: Additional context about what was being attempted
            recoverable: Whether the caller should retry or try alternatives
        
        Returns:
            Formatted error string for LLM consumption
        """
        error_code = self._exception_to_error_code(e)
        message = f"{context}: {str(e)}" if context else str(e)
        
        error = ToolError(
            message=message,
            tool_name=self.metadata.name,
            error_code=error_code,
            recoverable=recoverable,
            details=traceback.format_exc(),
        )
        return error.to_string()
    
    def _exception_to_error_code(self, e: Exception) -> str:
        """Map exception type to appropriate error code."""
        exception_name = type(e).__name__.lower()
        
        if "timeout" in exception_name:
            return ErrorCodes.TIMEOUT
        elif "connection" in exception_name or "network" in exception_name:
            return ErrorCodes.NETWORK_ERROR
        elif "rate" in exception_name or "limit" in exception_name:
            return ErrorCodes.RATE_LIMITED
        elif "auth" in exception_name or "permission" in exception_name:
            return ErrorCodes.API_KEY_INVALID
        elif "parse" in exception_name or "json" in exception_name:
            return ErrorCodes.PARSE_ERROR
        elif "validation" in exception_name or "value" in exception_name:
            return ErrorCodes.INVALID_PARAMS
        else:
            return ErrorCodes.EXTERNAL_SERVICE_ERROR
    
    # ============================================
    # Async/Sync Interoperability
    # ============================================
    
    def _run_async_in_sync(self, coro: Coroutine[None, None, str]) -> str:
        """Run an async coroutine from a sync context.
        
        Handles all the edge cases around event loops:
        - Running in an async context (e.g., FastAPI)
        - Running in a sync context with no loop
        - Running in a sync context with an existing loop
        """
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # We're in an async context, use thread pool to avoid blocking
                with ThreadPoolExecutor() as pool:
                    return pool.submit(asyncio.run, coro).result()
            return loop.run_until_complete(coro)
        except RuntimeError:
            return asyncio.run(coro)
    
    # ============================================
    # Core Execution Methods
    # ============================================
    
    @abstractmethod
    def _run(self, params: TParams) -> str:
        """Execute the tool synchronously with typed params.
        
        This is the main method to implement. Return a string result
        for the LLM to process.
        """
        ...
    
    def run_cached(self, params: TParams) -> str:
        """Execute the tool with caching support.
        
        Checks cache first, returns cached result if available.
        Otherwise executes _run and caches the result.
        """
        if not self.cache_enabled:
            return self._run(params)
        
        cache = get_tool_cache()
        tool_name = self.metadata.name
        
        # Check cache first
        if cached := cache.get(tool_name, params):
            return cached
        
        # Execute and cache result
        result = self._run(params)
        
        # Don't cache error responses (they start with "**Tool Error")
        if not result.startswith("**Tool Error"):
            cache.set(tool_name, params, result, self.cache_ttl)
        
        return result
    
    async def _arun(self, params: TParams, timeout: float = 30.0) -> str:
        """Execute async via thread pool with timeout (with caching)."""
        return await asyncio.wait_for(
            asyncio.to_thread(self.run_cached, params),
            timeout=timeout
        )
    
    # ============================================
    # Streaming Progress Support
    # ============================================
    
    def supports_streaming(self) -> bool:
        """Check if this tool supports progress streaming."""
        return self.metadata.supports_streaming
    
    async def stream_run(self, params: TParams) -> AsyncIterator[ToolProgress]:
        """Stream progress events during tool execution.
        
        Override this method in tools that support streaming to provide
        real-time progress updates. Default implementation just runs
        the tool and yields a complete event.
        
        Yields:
            ToolProgress events with status updates, data, and completion.
        """
        yield ToolProgress(kind="status", message="Starting...")
        try:
            result = await self._arun(params)
            yield ToolProgress(
                kind="complete", 
                message="Complete",
                percentage=100.0,
                data={"result": result}
            )
        except Exception as e:
            yield ToolProgress(
                kind="error",
                message=str(e),
                data={"error": str(e)}
            )
    
    async def _arun_with_progress(
        self, 
        params: TParams, 
        on_progress: ProgressCallback | None = None,
        timeout: float = 60.0
    ) -> str:
        """Execute async with progress callbacks.
        
        For tools that support streaming, collects all progress events
        and calls the callback for each. Returns the final result.
        """
        result = ""
        async def execute():
            nonlocal result
            async for progress in self.stream_run(params):
                if on_progress:
                    on_progress(progress)
                if progress.kind == "complete" and progress.data:
                    result = progress.data.get("result", "")
                elif progress.kind == "error":
                    raise Exception(progress.message)
            return result
        
        return await asyncio.wait_for(execute(), timeout=timeout)
    
    # ============================================
    # LangChain Integration
    # ============================================
    
    def to_langchain(self) -> StructuredTool:
        """Convert to LangChain StructuredTool with proper schema support."""
        tool_instance = self
        schema = self.params_schema
        
        def _invoke(**kwargs: object) -> str:
            return tool_instance.run_cached(schema(**kwargs))
        
        async def _ainvoke(**kwargs: object) -> str:
            return await tool_instance._arun(schema(**kwargs))
        
        return StructuredTool.from_function(
            func=_invoke,
            coroutine=_ainvoke,
            name=self.metadata.name,
            description=self.metadata.description,
            args_schema=schema,
        )


# ============================================
# Tool Registry
# ============================================

class ToolRegistry:
    """Central registry for all available tools."""
    
    __slots__ = ("_tools", "_langchain_cache")
    
    def __init__(self) -> None:
        self._tools: dict[str, BaseTool[BaseModel]] = {}
        self._langchain_cache: dict[str, StructuredTool] = {}
    
    def register(self, tool: BaseTool[BaseModel]) -> None:
        """Register a tool in the registry."""
        name = tool.metadata.name
        self._tools[name] = tool
        self._langchain_cache[name] = tool.to_langchain()
    
    def get(self, name: str) -> BaseTool[BaseModel] | None:
        """Get a tool by name."""
        return self._tools.get(name)
    
    def list_tools(self) -> list[ToolMetadata]:
        """List metadata for all enabled tools."""
        return [t.metadata for t in self._tools.values() if t.metadata.enabled]
    
    def list_by_category(self, category: str) -> list[ToolMetadata]:
        """List tools filtered by category."""
        return [t.metadata for t in self._tools.values() 
                if t.metadata.category == category and t.metadata.enabled]
    
    def get_langchain_tools(self) -> list[StructuredTool]:
        """Get all tools in LangChain format for agent use (cached)."""
        return [self._langchain_cache[n] for n, t in self._tools.items() 
                if t.metadata.enabled and n in self._langchain_cache]
    
    def get_tool_descriptions(self) -> str:
        """Get formatted descriptions of all tools for prompts."""
        return "\n".join(
            f"- **{t.metadata.name}** ({t.metadata.category}): {t.metadata.description}"
            for t in self._tools.values() if t.metadata.enabled
        )


# ============================================
# Global Registry Management
# ============================================

_registry: ToolRegistry | None = None
_initialized: bool = False


def get_registry() -> ToolRegistry:
    """Get the global tool registry (must call init_tools() first from main.py)."""
    global _registry
    if _registry is None:
        _registry = ToolRegistry()
    return _registry


def init_tools() -> ToolRegistry:
    """Initialize and register all tools. Call once from main.py at startup.
    
    Import and register all tools here to avoid circular imports.
    """
    global _initialized
    if _initialized:
        return get_registry()
    
    # Import tools here to avoid circular imports
    from .discovery import ToolDiscoveryTool
    # from .search import WebSearchTool
    # from .memory import MemoryTool
    # ... import other tools
    
    registry = get_registry()
    tools_to_register = [
        ToolDiscoveryTool(),
        # WebSearchTool(),
        # MemoryTool(),
        # ... other tools
    ]
    
    for tool in tools_to_register:
        registry.register(tool)
        log.debug(f"Registered tool: {tool.metadata.name}", extra={"category": tool.metadata.category})
    
    log.info(f"Tool registry initialized with {len(tools_to_register)} tools")
    _initialized = True
    return registry
```

---

### 2. cache.py - Tool Result Caching

```python
"""Request-scoped caching for tool results with TTL support.

Provides a simple in-memory cache with configurable TTL to prevent repeated
API calls for identical queries within a session.
"""
import hashlib
import json
import time
from dataclasses import dataclass
from typing import Any

from pydantic import BaseModel


DEFAULT_TTL_SECONDS = 300  # 5 minutes


@dataclass(slots=True)
class CacheEntry:
    """A cached tool result with expiration tracking."""
    value: str
    expires_at: float
    
    @property
    def is_expired(self) -> bool:
        return time.time() > self.expires_at


class ToolCache:
    """Thread-safe in-memory cache for tool results.
    
    Uses a simple dict with TTL-based expiration. Cache keys are generated
    from the tool name and a hash of the serialized parameters.
    
    Usage:
        cache = ToolCache()
        
        # Check cache first
        if cached := cache.get("web_search", params):
            return cached
        
        # Do the actual work
        result = expensive_api_call(params)
        
        # Store in cache
        cache.set("web_search", params, result)
        return result
    """
    
    __slots__ = ("_cache", "_ttl_seconds", "_max_entries")
    
    def __init__(self, ttl_seconds: float = DEFAULT_TTL_SECONDS, max_entries: int = 1000) -> None:
        self._cache: dict[str, CacheEntry] = {}
        self._ttl_seconds = ttl_seconds
        self._max_entries = max_entries
    
    def _make_key(self, tool_name: str, params: BaseModel | dict[str, Any]) -> str:
        """Generate a cache key from tool name and parameters."""
        if isinstance(params, BaseModel):
            params_dict = params.model_dump(mode="json")
        else:
            params_dict = params
        
        # Sort keys for consistent hashing
        params_json = json.dumps(params_dict, sort_keys=True, default=str)
        params_hash = hashlib.md5(params_json.encode()).hexdigest()[:12]
        
        return f"{tool_name}:{params_hash}"
    
    def get(self, tool_name: str, params: BaseModel | dict[str, Any]) -> str | None:
        """Get a cached result if it exists and hasn't expired."""
        key = self._make_key(tool_name, params)
        entry = self._cache.get(key)
        
        if entry is None:
            return None
        
        if entry.is_expired:
            del self._cache[key]
            return None
        
        return entry.value
    
    def set(
        self, 
        tool_name: str, 
        params: BaseModel | dict[str, Any], 
        value: str,
        ttl_seconds: float | None = None,
    ) -> None:
        """Store a result in the cache."""
        # Evict expired entries if cache is getting large
        if len(self._cache) >= self._max_entries:
            self._evict_expired()
        
        # If still too large, evict oldest entries
        if len(self._cache) >= self._max_entries:
            self._evict_oldest(max(1, self._max_entries // 4))
        
        key = self._make_key(tool_name, params)
        ttl = ttl_seconds if ttl_seconds is not None else self._ttl_seconds
        
        self._cache[key] = CacheEntry(
            value=value,
            expires_at=time.time() + ttl,
        )
    
    def invalidate(self, tool_name: str, params: BaseModel | dict[str, Any]) -> bool:
        """Remove a specific entry from the cache."""
        key = self._make_key(tool_name, params)
        if key in self._cache:
            del self._cache[key]
            return True
        return False
    
    def invalidate_tool(self, tool_name: str) -> int:
        """Remove all cached entries for a specific tool."""
        prefix = f"{tool_name}:"
        keys_to_remove = [k for k in self._cache if k.startswith(prefix)]
        for key in keys_to_remove:
            del self._cache[key]
        return len(keys_to_remove)
    
    def clear(self) -> None:
        """Clear the entire cache."""
        self._cache.clear()
    
    def _evict_expired(self) -> int:
        """Remove all expired entries."""
        expired_keys = [k for k, v in self._cache.items() if v.is_expired]
        for key in expired_keys:
            del self._cache[key]
        return len(expired_keys)
    
    def _evict_oldest(self, count: int) -> None:
        """Remove the oldest N entries based on expiration time."""
        if not self._cache:
            return
        
        sorted_keys = sorted(
            self._cache.keys(), 
            key=lambda k: self._cache[k].expires_at
        )
        
        for key in sorted_keys[:count]:
            del self._cache[key]
    
    @property
    def size(self) -> int:
        """Current number of entries in the cache."""
        return len(self._cache)
    
    def stats(self) -> dict[str, Any]:
        """Get cache statistics for monitoring."""
        now = time.time()
        expired_count = sum(1 for v in self._cache.values() if v.is_expired)
        
        return {
            "total_entries": len(self._cache),
            "expired_entries": expired_count,
            "active_entries": len(self._cache) - expired_count,
            "ttl_seconds": self._ttl_seconds,
            "max_entries": self._max_entries,
        }


# Global cache instance - shared across all tools
_tool_cache: ToolCache | None = None


def get_tool_cache() -> ToolCache:
    """Get the global tool cache instance."""
    global _tool_cache
    if _tool_cache is None:
        _tool_cache = ToolCache()
    return _tool_cache


def reset_tool_cache() -> None:
    """Reset the global cache (useful for testing)."""
    global _tool_cache
    if _tool_cache is not None:
        _tool_cache.clear()
    _tool_cache = None
```

---

### 3. discovery/discovery_tool.py - Tool Discovery Tool

```python
"""Tool discovery tool - allows the agent to list available tools."""
from typing import ClassVar, Literal

from pydantic import Field, BaseModel

from ..base import BaseTool, ToolMetadata, get_registry


class DiscoveryParams(BaseModel):
    """Parameters for tool discovery."""
    category: str | None = Field(
        default=None,
        description="Filter tools by category (e.g., 'search', 'career', 'memory'). Leave empty to list all."
    )
    format: Literal["brief", "detailed"] = Field(
        default="brief",
        description="Output format: 'brief' for names only, 'detailed' for full descriptions"
    )


class ToolDiscoveryTool(BaseTool[DiscoveryParams]):
    """Meta-tool that lists all available tools.
    
    This tool allows the agent to discover what capabilities are available,
    helping it decide which tool to use for a given task.
    """
    
    metadata: ClassVar[ToolMetadata] = ToolMetadata(
        name="discover_tools",
        description="List all available tools and their capabilities. Use this to discover what "
                    "tools you can use to help the user. Can filter by category.",
        category="meta",
        requires_api_key=False,
        enabled=True,
    )
    params_schema: ClassVar[type[DiscoveryParams]] = DiscoveryParams
    
    # Discovery tool should not be cached (always reflect current state)
    cache_enabled: ClassVar[bool] = False
    
    def _run(self, params: DiscoveryParams) -> str:
        registry = get_registry()
        
        # Get tools (filtered by category if specified)
        if params.category:
            tools = registry.list_by_category(params.category)
        else:
            tools = registry.list_tools()
        
        if not tools:
            if params.category:
                return f"No tools found in category '{params.category}'. Try without a category filter."
            return "No tools are currently available."
        
        # Format output
        if params.format == "brief":
            return self._format_brief(tools, params.category)
        else:
            return self._format_detailed(tools, params.category)
    
    def _format_brief(self, tools: list[ToolMetadata], category: str | None) -> str:
        """Brief format: just names and one-line descriptions."""
        header = f"**Available Tools" + (f" in '{category}'**" if category else "**")
        lines = [header, ""]
        
        # Group by category
        by_category: dict[str, list[ToolMetadata]] = {}
        for tool in tools:
            by_category.setdefault(tool.category, []).append(tool)
        
        for cat, cat_tools in sorted(by_category.items()):
            lines.append(f"**{cat.title()}:**")
            for tool in cat_tools:
                api_note = " âš¡" if tool.requires_api_key else ""
                stream_note = " ðŸ“¡" if tool.supports_streaming else ""
                lines.append(f"- `{tool.name}`{api_note}{stream_note}: {tool.description[:80]}...")
            lines.append("")
        
        lines.append("_âš¡ = requires API key | ðŸ“¡ = supports streaming progress_")
        return "\n".join(lines)
    
    def _format_detailed(self, tools: list[ToolMetadata], category: str | None) -> str:
        """Detailed format: full descriptions and metadata."""
        header = f"**Available Tools" + (f" in '{category}'**" if category else "**")
        lines = [header, ""]
        
        for tool in sorted(tools, key=lambda t: (t.category, t.name)):
            lines.append(f"### {tool.name}")
            lines.append(f"**Category:** {tool.category}")
            lines.append(f"**Description:** {tool.description}")
            if tool.requires_api_key:
                lines.append("**Note:** Requires API key to function")
            if tool.supports_streaming:
                lines.append("**Feature:** Supports real-time progress streaming")
            lines.append("")
        
        return "\n".join(lines)
```

---

### 4. __init__.py - Public Exports

```python
"""Tool System - Extensible tool registry for AI agents."""
from .base import (
    BaseTool, 
    ToolMetadata, 
    ToolRegistry, 
    EmptyParams, 
    ToolError,
    ErrorCodes,
    ToolProgress,
    ProgressCallback,
    get_registry, 
    init_tools,
)
from .cache import (
    ToolCache,
    get_tool_cache,
    reset_tool_cache,
    DEFAULT_TTL_SECONDS,
)
from .discovery import ToolDiscoveryTool
from .discovery.discovery_tool import DiscoveryParams

__all__ = [
    # Base
    "BaseTool",
    "ToolMetadata",
    "ToolRegistry",
    "EmptyParams",
    "ToolError",
    "ErrorCodes",
    "ToolProgress",
    "ProgressCallback",
    "get_registry",
    "init_tools",
    # Cache
    "ToolCache",
    "get_tool_cache",
    "reset_tool_cache",
    "DEFAULT_TTL_SECONDS",
    # Discovery
    "ToolDiscoveryTool",
    "DiscoveryParams",
]
```

---

## Creating a New Tool

### Step 1: Create Tool File

Create `app/tools/[category]/[tool_name].py`:

```python
"""Description of what this tool does."""
from typing import ClassVar

from pydantic import Field, BaseModel

from ..base import BaseTool, ToolMetadata, ErrorCodes


class MyToolParams(BaseModel):
    """Parameters for my tool."""
    query: str = Field(..., description="The search query")
    limit: int = Field(default=5, ge=1, le=20, description="Max results")


class MyTool(BaseTool[MyToolParams]):
    """Tool description for documentation."""
    
    metadata: ClassVar[ToolMetadata] = ToolMetadata(
        name="my_tool",                          # snake_case, unique
        description="Description for LLM",        # What does this tool do?
        category="search",                        # Grouping category
        requires_api_key=False,                   # Does it need external API?
        enabled=True,                             # Can disable without removing
        supports_streaming=False,                 # Does it stream progress?
    )
    params_schema: ClassVar[type[MyToolParams]] = MyToolParams
    
    # Optional: Override caching behavior
    # cache_enabled: ClassVar[bool] = False
    # cache_ttl: ClassVar[float] = 60  # seconds
    
    def _run(self, params: MyToolParams) -> str:
        """Main execution logic."""
        try:
            # Your implementation here
            result = f"Found results for: {params.query}"
            return result
        except Exception as e:
            return self._create_error_from_exception(e, "Search failed")
```

### Step 2: Create Category __init__.py

Create `app/tools/[category]/__init__.py`:

```python
"""[Category] tools."""
from .my_tool import MyTool, MyToolParams

__all__ = ["MyTool", "MyToolParams"]
```

### Step 3: Register in init_tools()

Update `app/tools/base.py`:

```python
def init_tools() -> ToolRegistry:
    from .discovery import ToolDiscoveryTool
    from .my_category import MyTool  # Add import
    
    registry = get_registry()
    tools_to_register = [
        ToolDiscoveryTool(),
        MyTool(),  # Add registration
    ]
    # ...
```

### Step 4: Export in __init__.py

Update `app/tools/__init__.py`:

```python
from .my_category import MyTool, MyToolParams

__all__ = [
    # ... existing
    "MyTool",
    "MyToolParams",
]
```

---

## Tool Patterns

### Pattern: Async Tool with External API

```python
import os
from typing import ClassVar

from pydantic import Field, BaseModel

from ..base import BaseTool, ToolMetadata, ErrorCodes


class ApiToolParams(BaseModel):
    query: str = Field(..., description="Query string")


class ApiTool(BaseTool[ApiToolParams]):
    """Tool that calls an external API."""
    
    metadata: ClassVar[ToolMetadata] = ToolMetadata(
        name="api_tool",
        description="Fetches data from external API",
        category="external",
        requires_api_key=True,
        enabled=True,
    )
    params_schema: ClassVar[type[ApiToolParams]] = ApiToolParams
    
    def _run(self, params: ApiToolParams) -> str:
        """Sync wrapper for async implementation."""
        return self._run_async_in_sync(self._async_run(params))
    
    async def _async_run(self, params: ApiToolParams) -> str:
        """Actual async implementation."""
        api_key = os.environ.get("MY_API_KEY")
        
        if not api_key:
            return self._create_error(
                "API key not configured",
                ErrorCodes.API_KEY_MISSING,
                recoverable=False,
            )
        
        try:
            import httpx
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    "https://api.example.com/search",
                    params={"q": params.query},
                    headers={"Authorization": f"Bearer {api_key}"},
                )
                response.raise_for_status()
                return self._format_results(response.json())
        except httpx.TimeoutException:
            return self._create_error("Request timed out", ErrorCodes.TIMEOUT)
        except httpx.HTTPError as e:
            return self._create_error_from_exception(e, "API request failed")
    
    def _format_results(self, data: dict) -> str:
        """Format API response for LLM."""
        # ... formatting logic
        return "Formatted results"
```

### Pattern: Streaming Progress Tool

```python
import asyncio
from typing import AsyncIterator, ClassVar

from pydantic import Field, BaseModel

from ..base import BaseTool, ToolMetadata, ToolProgress


class StreamingToolParams(BaseModel):
    items: list[str] = Field(..., description="Items to process")


class StreamingTool(BaseTool[StreamingToolParams]):
    """Tool that streams progress for long operations."""
    
    metadata: ClassVar[ToolMetadata] = ToolMetadata(
        name="streaming_tool",
        description="Processes items with progress updates",
        category="processing",
        requires_api_key=False,
        enabled=True,
        supports_streaming=True,  # Enable streaming
    )
    params_schema: ClassVar[type[StreamingToolParams]] = StreamingToolParams
    
    def _run(self, params: StreamingToolParams) -> str:
        """Sync version - just process without progress."""
        results = []
        for item in params.items:
            results.append(f"Processed: {item}")
        return "\n".join(results)
    
    async def stream_run(self, params: StreamingToolParams) -> AsyncIterator[ToolProgress]:
        """Stream progress events during execution."""
        total = len(params.items)
        results = []
        
        yield ToolProgress(
            kind="status",
            message=f"Starting to process {total} items...",
            step_number=0,
            total_steps=total,
            percentage=0.0,
        )
        
        for i, item in enumerate(params.items):
            # Simulate processing
            await asyncio.sleep(0.1)
            results.append(f"Processed: {item}")
            
            yield ToolProgress(
                kind="step",
                message=f"Processed: {item}",
                step_number=i + 1,
                total_steps=total,
                percentage=((i + 1) / total) * 100,
                data={"item": item, "index": i}
            )
        
        final_result = "\n".join(results)
        yield ToolProgress(
            kind="complete",
            message=f"Completed processing {total} items",
            percentage=100.0,
            data={"result": final_result}
        )
```

### Pattern: Tool with Fallback

```python
from typing import ClassVar

from pydantic import Field, BaseModel

from ..base import BaseTool, ToolMetadata, ErrorCodes


class FallbackToolParams(BaseModel):
    query: str = Field(..., description="Search query")


class FallbackTool(BaseTool[FallbackToolParams]):
    """Tool that falls back to alternative sources."""
    
    metadata: ClassVar[ToolMetadata] = ToolMetadata(
        name="fallback_tool",
        description="Searches with fallback to alternative sources",
        category="search",
        requires_api_key=True,
        enabled=True,
    )
    params_schema: ClassVar[type[FallbackToolParams]] = FallbackToolParams
    
    def _run(self, params: FallbackToolParams) -> str:
        # Try primary source
        result = self._try_primary(params.query)
        if result:
            return result
        
        # Fallback to secondary
        result = self._try_secondary(params.query)
        if result:
            return f"(via fallback)\n{result}"
        
        return self._create_error(
            f"No results found for: {params.query}",
            ErrorCodes.NO_RESULTS,
            recoverable=True,
        )
    
    def _try_primary(self, query: str) -> str | None:
        """Try primary data source."""
        # ... implementation
        return None
    
    def _try_secondary(self, query: str) -> str | None:
        """Try fallback data source."""
        # ... implementation
        return None
```

---

## LLM Integration

### Agent Executor Setup

```python
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI

from app.tools import get_registry


def create_agent_executor(system_prompt: str) -> AgentExecutor:
    """Create a LangChain agent with all registered tools."""
    registry = get_registry()
    tools = registry.get_langchain_tools()
    
    if not tools:
        return None
    
    model = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])
    
    agent = create_tool_calling_agent(model, tools, prompt)
    return AgentExecutor(
        agent=agent, 
        tools=tools, 
        verbose=False, 
        handle_parsing_errors=True
    )
```

### Streaming Tool Events

```python
async def stream_with_tool_events(
    executor: AgentExecutor,
    message: str,
    history: list,
) -> AsyncIterator[dict]:
    """Stream agent response with tool status events."""
    async for event in executor.astream_events(
        {"input": message, "chat_history": history},
        version="v2",
    ):
        kind = event["event"]
        
        if kind == "on_tool_start":
            tool_name = event.get("name", "unknown")
            yield {"type": "tool_start", "tool": tool_name}
            
        elif kind == "on_tool_end":
            tool_name = event.get("name", "unknown")
            yield {"type": "tool_end", "tool": tool_name}
            
        elif kind == "on_chat_model_stream":
            chunk = event.get("data", {}).get("chunk")
            if chunk and hasattr(chunk, "content") and isinstance(chunk.content, str):
                yield {"type": "token", "content": chunk.content}
```

---

## Testing Tools

```python
"""Test template for tools."""
import pytest
from app.tools import get_registry, reset_tool_cache
from app.tools.my_category import MyTool, MyToolParams


class TestMyTool:
    """Tests for MyTool."""
    
    @pytest.fixture(autouse=True)
    def reset_cache(self):
        """Reset cache before each test."""
        reset_tool_cache()
    
    @pytest.fixture
    def tool(self) -> MyTool:
        return MyTool()
    
    def test_metadata(self, tool: MyTool):
        """Test tool metadata is correct."""
        assert tool.metadata.name == "my_tool"
        assert tool.metadata.category == "search"
        assert tool.metadata.enabled is True
    
    def test_basic_execution(self, tool: MyTool):
        """Test basic tool execution."""
        params = MyToolParams(query="test query")
        result = tool._run(params)
        assert "test query" in result
        assert "**Tool Error" not in result
    
    def test_caching(self, tool: MyTool):
        """Test that results are cached."""
        params = MyToolParams(query="cached query")
        
        # First call
        result1 = tool.run_cached(params)
        
        # Second call should hit cache
        result2 = tool.run_cached(params)
        
        assert result1 == result2
    
    def test_error_handling(self, tool: MyTool):
        """Test error handling."""
        # Test with invalid input that triggers error
        params = MyToolParams(query="")
        result = tool._run(params)
        # Verify error format or handle gracefully
    
    @pytest.mark.asyncio
    async def test_async_execution(self, tool: MyTool):
        """Test async execution."""
        params = MyToolParams(query="async test")
        result = await tool._arun(params)
        assert "async test" in result


class TestToolRegistry:
    """Tests for tool registration."""
    
    def test_tool_registered(self):
        """Test that MyTool is in the registry."""
        registry = get_registry()
        tool = registry.get("my_tool")
        assert tool is not None
        assert isinstance(tool, MyTool)
    
    def test_langchain_conversion(self):
        """Test LangChain tool conversion."""
        registry = get_registry()
        lc_tools = registry.get_langchain_tools()
        names = [t.name for t in lc_tools]
        assert "my_tool" in names
```

---

## Checklist for New Tools

- [ ] Create tool file in appropriate category directory
- [ ] Define `Params` model with Field descriptions
- [ ] Implement tool class extending `BaseTool[Params]`
- [ ] Set `metadata` class variable with all fields
- [ ] Set `params_schema` class variable
- [ ] Implement `_run()` method
- [ ] Use `_create_error()` for error handling
- [ ] Add async support if needed (`_async_run`)
- [ ] Add streaming support if long-running (`stream_run`)
- [ ] Configure caching (`cache_enabled`, `cache_ttl`)
- [ ] Create category `__init__.py` if new category
- [ ] Register tool in `init_tools()`
- [ ] Export in main `__init__.py`
- [ ] Write tests
- [ ] Test with actual agent execution

---

## Category Suggestions

Organize tools by purpose:

| Category | Purpose | Examples |
|----------|---------|----------|
| `meta` | System tools | discover_tools |
| `search` | Web/data search | web_search |
| `memory` | Persistence | memory |
| `career` | Career tools | jobs_search, interview_prep |
| `research` | Academic/cited | cited_research |
| `media` | Content search | youtube_search, image_search |
| `local` | Location-aware | local_jobs, local_colleges |
| `external` | External APIs | Company research, etc |

---

This document provides everything needed to rebuild the tool system from scratch.
